{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ecf9f5-e64f-405f-bc90-876f90c68164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'relevance': 100,\n",
       "  'state': 'TX',\n",
       "  'bill_number': 'HB5127',\n",
       "  'bill_id': 1734803,\n",
       "  'change_hash': '6a00e7541e5947ea17adea3470734c9c',\n",
       "  'url': 'https://legiscan.com/TX/bill/HB5127/2023',\n",
       "  'text_url': 'https://legiscan.com/TX/text/HB5127/2023',\n",
       "  'research_url': 'https://legiscan.com/TX/research/HB5127/2023',\n",
       "  'last_action_date': '2023-03-24',\n",
       "  'last_action': 'Referred to Higher Education',\n",
       "  'title': 'Relating to public higher education reform; authorizing administrative penalties.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.legiscan import paged_search\n",
    "\n",
    "list(paged_search('TX', 'H5127', max_pages=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bff70bb-76ef-4aea-b392-5cfef9229f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegiscan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m paged_search\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msearch_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaged_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5127\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbill_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mH5127\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lib.legiscan import paged_search\n",
    "\n",
    "next(search_result for search_result in paged_search('TX', '5127', max_pages=1000) if search_result['bill_number'] == 'H5127')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b8eea14-9279-4591-9823-4085ca274a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page 1\n",
      "Searching page 2\n",
      "Searching page 3\n",
      "Searching page 4\n",
      "Searching page 5\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 46\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chain(\n\u001b[1;32m     37\u001b[0m         extract_results(first_page),\n\u001b[1;32m     38\u001b[0m         chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# next(search_result for search_result in paged_search('TX', '8') if search_result['bill_number'] == 'SB2576')\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msearch_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaged_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m08\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbill_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHB4278\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# list(paged_search('TX', '8'))\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sys\n",
    "\n",
    "from itertools import chain, islice\n",
    "\n",
    "from lib.legiscan import legiscan_api\n",
    "\n",
    "LEGISCAN_API_URL = 'http://api.legiscan.com'\n",
    "\n",
    "def extract_results(search_response):\n",
    "    search_result = search_response['searchresult']\n",
    "    return (\n",
    "        search_result[key]\n",
    "        for key\n",
    "        in sorted(search_result.keys() - ['summary'], key=lambda x: int(x))\n",
    "    )\n",
    "\n",
    "@legiscan_api\n",
    "def simple_search(state: str, search_string: str, api_key: str, session, page: int = 1):\n",
    "    print(f'Searching page {page}')\n",
    "    assembled_params = {\n",
    "        'state': state,\n",
    "        'query': search_string,\n",
    "        'op': 'getSearch',\n",
    "        'key': api_key,\n",
    "        'page': page,\n",
    "    }\n",
    "    result = session.get(LEGISCAN_API_URL, params=assembled_params)\n",
    "    return result.json()\n",
    "\n",
    "@legiscan_api\n",
    "def paged_search(state: str, search_string: str, api_key: str, session, max_pages: int = sys.maxsize):\n",
    "    first_page = simple_search(state, search_string, page=1)\n",
    "    num_pages = first_page['searchresult']['summary']['page_total']\n",
    "    \n",
    "    return chain(\n",
    "        extract_results(first_page),\n",
    "        chain.from_iterable(\n",
    "            extract_results(simple_search(state, search_string, page=page_num+1))\n",
    "            for page_num\n",
    "            in range(1, min(num_pages, max_pages))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# next(search_result for search_result in paged_search('TX', '8') if search_result['bill_number'] == 'SB2576')\n",
    "next(search_result for search_result in paged_search('TX', '08', max_pages=5) if search_result['bill_number'] == 'HB4278')\n",
    "# list(paged_search('TX', '8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233c2e63-edf4-4026-af5a-4c91aa92a933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amy/Library/Caches/pypoetry/virtualenvs/tthb-IZuJWOhf-py3.9/lib/python3.9/site-packages/pandas/core/dtypes/astype.py:189: RuntimeWarning: invalid value encountered in cast\n",
      "  return values.astype(dtype, copy=copy)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>liposuction lipofilling voice surgery thyroid ...</td>\n",
       "      <td>31</td>\n",
       "      <td>2.249253e+32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mammoplasty facial feminization surgery liposu...</td>\n",
       "      <td>33</td>\n",
       "      <td>2.085505e+32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lipofilling voice surgery thyroid cartilage re...</td>\n",
       "      <td>31</td>\n",
       "      <td>1.233965e+32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>augmentation mammoplasty facial feminization s...</td>\n",
       "      <td>33</td>\n",
       "      <td>9.176220e+31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feminization surgery liposuction lipofilling v...</td>\n",
       "      <td>33</td>\n",
       "      <td>4.506331e+31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>of the state of missouri a follow section a ch...</td>\n",
       "      <td>30</td>\n",
       "      <td>6.532033e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>the state or any agency officer or employee of...</td>\n",
       "      <td>21</td>\n",
       "      <td>3.718365e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>be it enact by the people of the state of</td>\n",
       "      <td>56</td>\n",
       "      <td>3.306950e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>by the legislature of the state of texas secti...</td>\n",
       "      <td>25</td>\n",
       "      <td>2.218322e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>be it enact by the legislature of the state of</td>\n",
       "      <td>175</td>\n",
       "      <td>7.684254e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0    1             2\n",
       "0    liposuction lipofilling voice surgery thyroid ...   31  2.249253e+32\n",
       "1    mammoplasty facial feminization surgery liposu...   33  2.085505e+32\n",
       "2    lipofilling voice surgery thyroid cartilage re...   31  1.233965e+32\n",
       "3    augmentation mammoplasty facial feminization s...   33  9.176220e+31\n",
       "4    feminization surgery liposuction lipofilling v...   33  4.506331e+31\n",
       "..                                                 ...  ...           ...\n",
       "742  of the state of missouri a follow section a ch...   30  6.532033e+08\n",
       "743  the state or any agency officer or employee of...   21  3.718365e+08\n",
       "744          be it enact by the people of the state of   56  3.306950e+08\n",
       "745  by the legislature of the state of texas secti...   25  2.218322e+08\n",
       "746     be it enact by the legislature of the state of  175  7.684254e+07\n",
       "\n",
       "[747 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_json('../tmp/prob_sorted_ngrams.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c817143b-3389-4718-8287-c45a06049354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import chain, repeat\n",
    "from nltk import FreqDist\n",
    "\n",
    "from lib.util import load_json, write_json\n",
    "\n",
    "raw_tokens = chain(\n",
    "    load_json('../tmp/raw_tokens.json'),\n",
    "    load_json('../tmp/neutral_corpus/tokens.json'),\n",
    ")\n",
    "\n",
    "dist = FreqDist(raw_tokens)\n",
    "\n",
    "token_count = reduce(lambda x, y: x+y, dist.values(), 0)\n",
    "\n",
    "probs = {\n",
    "    k: v/token_count\n",
    "    for k, v\n",
    "    in dist.items()\n",
    "}\n",
    "\n",
    "def est_phrase_probability(phrase, probs):\n",
    "    return reduce(lambda x, y: x * probs[y], phrase, 1)\n",
    "\n",
    "ngrams = load_json('../tmp/grams-10.json')\n",
    "tagged_ngrams = {\n",
    "    ngram: (\n",
    "        len(occurrences), \n",
    "        (\n",
    "            est_phrase_probability(repeat('the', len(ngram.split(' '))), probs)\n",
    "            / est_phrase_probability(ngram.split(' '), probs)\n",
    "        ),\n",
    "    )\n",
    "    for ngram, occurrences\n",
    "    in ngrams.items()\n",
    "}\n",
    "\n",
    "prob_sorted_ngrams = [\n",
    "    [ngram, *computed]\n",
    "    for ngram, computed\n",
    "    in sorted(tagged_ngrams.items(), key=lambda x: x[1][1], reverse=True)\n",
    "]\n",
    "write_json(prob_sorted_ngrams, '../tmp/prob_sorted_ngrams.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92667deb-c7aa-4684-9009-3f5388c2b117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "from pprint import pprint\n",
    "\n",
    "from lib.tasks import tokenize_corpus\n",
    "from lib.util import load_json\n",
    "\n",
    "stopwords = set(load_json('../artifacts/legal_stopwords.json'))\n",
    "\n",
    "#raw_tokens = list(tokenize_corpus('../bills/*'))\n",
    "\n",
    "import json\n",
    "\n",
    "# with open('../tmp/raw_tokens.json', 'w') as f:\n",
    "#     json.dump(raw_tokens, f)\n",
    "\n",
    "with open('../tmp/raw_tokens.json', 'r') as f:\n",
    "    raw_tokens = json.load(f)\n",
    "\n",
    "filtered_tokens = [\n",
    "    token\n",
    "    for token\n",
    "    in raw_tokens\n",
    "    if len(token) > 1\n",
    "]\n",
    "\n",
    "for i in range(2,51):\n",
    "    grams = ngrams(filtered_tokens, i)\n",
    "    dist = FreqDist(grams)\n",
    "    \n",
    "    serializable = {\n",
    "        ' '.join(k): v\n",
    "        for k, v\n",
    "        in dist.items()\n",
    "        if v > 20\n",
    "    }\n",
    "\n",
    "    with open(f'../tmp/ngrams-{str(i).zfill(2)}.json', 'w') as f:\n",
    "        json.dump(serializable, f, indent=2)\n",
    "        \n",
    "# tengrams = ngrams(list(filtered_tokens), 10)\n",
    "# dist = FreqDist(tengrams)\n",
    "\n",
    "# # pprint(sorted([\n",
    "# #     (k, v)\n",
    "# #     for k, v in dist.items()\n",
    "# #     if v > 60\n",
    "# # ], key=lambda x: x[1], reverse=True))\n",
    "\n",
    "\n",
    "# serializable = {\n",
    "#     ' '.join(k): v\n",
    "#     for k, v\n",
    "#     in dist.items()\n",
    "#     if v > 20\n",
    "# }\n",
    "\n",
    "# with open('../tmp/tengrams.json', 'w') as f:\n",
    "#     json.dump(serializable, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cf193-f4c6-49b2-9614-5c91bbbc8cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1700313\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "summaries = pd.read_json('../tmp/neutral_corpus/summaries.json')\n",
    "summaries.loc[summaries.legiscan_bill_id == 1700313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c35219-04ce-4574-99a5-d31ae0a56179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pprint import pprint\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_pdf_tokens_pypdf2(file_path: str) -> str:\n",
    "    reader = PdfReader(file_path)\n",
    "    return reader.pages[0].extract_text()\n",
    "\n",
    "def extract_pdf_tokens_pdfplumber(file_path: str) -> str:\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        return pdf.pages[0].extract_text()\n",
    "\n",
    "pprint(extract_pdf_tokens_pdfplumber('../archive/bills/OK_SB973.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b3a90-0508-4d68-888e-07de4a226682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "\n",
    "def find_hyphenate(text: str):\n",
    "    return re.subn(r'(\\w)+-\\s*\\n\\s*(\\w+)', r'\\g<1>\\g<2>\\n', text, re.MULTILINE)\n",
    "\n",
    "sample = 'the quick the \\n brown fox jumps o- \\n ver the \\n lazy dog'\n",
    "print(find_hyphenate(sample))\n",
    "\n",
    "re.search(r'(\\w)+-\\s*\\n\\s+(\\w+)', sample).groups()\n",
    "# glob('../tmp/neutral_corpus/bills/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7688789-a5f5-43f1-b9b5-9e6f6e3a29c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from collections import namedtuple\n",
    "import json\n",
    "from retrieval.legiscan import legiscan_auth\n",
    "\n",
    "Dataset = namedtuple(\"Dataset\", \"state year session modified exported json csv\")\n",
    "\n",
    "@legiscan_auth\n",
    "def enumerate_datasets(session):\n",
    "    return session.get('https://legiscan.com/datasets').text\n",
    "\n",
    "soup = Soup(enumerate_datasets())\n",
    "dataset_table = soup.find(id='gaits-datasets')\n",
    "table_data = [\n",
    "    Dataset(*(\n",
    "        cell.text if len(cell.find_all('a'))<1 else cell.find_all('a')[0].attrs['href']\n",
    "        for cell \n",
    "        in row.find_all('td')))\n",
    "    for row\n",
    "    in dataset_table.find_all('tbody')[0].find_all('tr')\n",
    "]\n",
    "\n",
    "len([item.json for item in table_data if '2023' in item.session])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65f9e4-5f48-438f-82ee-e0d208f26e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../datasets/geography.json', 'r') as f:\n",
    "    geography = json.load(f)\n",
    "    \n",
    "pairs = [\n",
    "('ALABAMA', 'AL'),\n",
    "('ALASKA', 'AK'),\n",
    "('AMERICAN SAMOA', 'AS'),\n",
    "('ARIZONA', 'AZ'),\n",
    "('ARKANSAS', 'AR'),\n",
    "('CALIFORNIA', 'CA'),\n",
    "('COLORADO', 'CO'),\n",
    "('CONNECTICUT', 'CT'),\n",
    "('DELAWARE', 'DE'),\n",
    "('DISTRICT OF COLUMBIA', 'DC'),\n",
    "('FLORIDA', 'FL'),\n",
    "('GEORGIA', 'GA'),\n",
    "('GUAM', 'GU'),\n",
    "('HAWAII', 'HI'),\n",
    "('IDAHO', 'ID'),\n",
    "('ILLINOIS', 'IL'),\n",
    "('INDIANA', 'IN'),\n",
    "('IOWA', 'IA'),\n",
    "('KANSAS', 'KS'),\n",
    "('KENTUCKY', 'KY'),\n",
    "('LOUISIANA', 'LA'),\n",
    "('MAINE', 'ME'),\n",
    "('MARYLAND', 'MD'),\n",
    "('MASSACHUSETTS', 'MA'),\n",
    "('MICHIGAN', 'MI'),\n",
    "('MINNESOTA', 'MN'),\n",
    "('MISSISSIPPI', 'MS'),\n",
    "('MISSOURI', 'MO'),\n",
    "('MONTANA', 'MT'),\n",
    "('NEBRASKA', 'NE'),\n",
    "('NEVADA', 'NV'),\n",
    "('NEW HAMPSHIRE', 'NH'),\n",
    "('NEW JERSEY', 'NJ'),\n",
    "('NEW MEXICO', 'NM'),\n",
    "('NEW YORK', 'NY'),\n",
    "('NORTH CAROLINA', 'NC'),\n",
    "('NORTH DAKOTA', 'ND'),\n",
    "('NORTHERN MARIANA IS', 'MP'),\n",
    "('OHIO', 'OH'),\n",
    "('OKLAHOMA', 'OK'),\n",
    "('OREGON', 'OR'),\n",
    "('PENNSYLVANIA', 'PA'),\n",
    "('PUERTO RICO', 'PR'),\n",
    "('RHODE ISLAND', 'RI'),\n",
    "('SOUTH CAROLINA', 'SC'),\n",
    "('SOUTH DAKOTA', 'SD'),\n",
    "('TENNESSEE', 'TN'),\n",
    "('TEXAS', 'TX'),\n",
    "('UNITED STATES', 'US'),\n",
    "('UTAH', 'UT'),\n",
    "('VERMONT', 'VT'),\n",
    "('VIRGINIA', 'VA'),\n",
    "('VIRGIN ISLANDS', 'VI'),\n",
    "('WASHINGTON', 'WA'),\n",
    "('WEST VIRGINIA', 'WV'),\n",
    "('WISCONSIN', 'WI'),\n",
    "('WYOMING', 'WY'),\n",
    "]\n",
    "\n",
    "def fix_case(state: str):\n",
    "    return ' '.join(word.capitalize() for word in state.split(' '))\n",
    "\n",
    "geography['state_abbreviations'] = {fix_case(tup[0]): tup[1] for tup in pairs}\n",
    "geography['state_names'] = {tup[1]: fix_case(tup[0]) for tup in pairs}\n",
    "\n",
    "with open('../datasets/geography.json', 'w') as f:\n",
    "    json.dump(geography, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3373d-f7a8-4a36-9d24-d3ac1ebc56a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from operator import itemgetter\n",
    "import json\n",
    "\n",
    "metas =  glob.glob('../tmp/legiscan/*meta*.json')\n",
    "def get_meta(meta_name):\n",
    "    with open(meta_name, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def summarize(meta):\n",
    "    bill = meta['bill']\n",
    "    state, bill_id, status_date, legiscan_bill_id = itemgetter('state', 'bill_number', 'status_date', 'bill_id')(bill)\n",
    "    \n",
    "    return {\n",
    "        'state': state,\n",
    "        'bill_id': bill_id,\n",
    "        'status_date': status_date,\n",
    "        'legiscan_bill_id': legiscan_bill_id,\n",
    "    }\n",
    "\n",
    "[summarize(get_meta(meta)) for meta in metas][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d658fa8-fb45-41c2-942b-edc122e164ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from itertools import chain, islice, takewhile\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "\n",
    "TRACKER_URL = 'https://www.equalitytexas.org/legislature/legislative-bill-tracker-2023'\n",
    "OUTPUT_PATH = '../datasets/equalitytexas.json'\n",
    "\n",
    "def extract_row(row):\n",
    "    cells = row.find_all('td')\n",
    "    d = re.search(r'\\d{2}/\\d{2}/\\d{4}', cells[3].text)\n",
    "    return {\n",
    "        'state': 'TX',\n",
    "        'bill_id': cells[0].text,\n",
    "        'sponsors': [sponsor for sponsor in cells[1].text.split(' ') if sponsor not in string.punctuation],\n",
    "        'description': cells[2].text,\n",
    "        'status_date': d.group(0) if d else '',\n",
    "    }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "page = requests.get(TRACKER_URL)\n",
    "\n",
    "soup = Soup(page.content, 'html.parser')\n",
    "bad_bills = soup.find(id='bad-bills')\n",
    "\n",
    "bill_tables = islice(bad_bills.parent.parent.find_all('table'), 1, None)\n",
    "relevant_rows = chain.from_iterable(\n",
    "    (row for row in tbl.find_all('tr') if not row.find('th'))\n",
    "    for tbl\n",
    "    in bill_tables\n",
    ")\n",
    "\n",
    "dataset = list((takewhile(lambda r: r['bill_id'] != '#N/A', (extract_row(row) for row in relevant_rows))))\n",
    "\n",
    "with open(OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Data \"equalitytexas.json\" refreshed with {len(dataset)} items ({(end_time-start_time):.2f}s elapsed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add61f42-c18d-46d7-a44b-a4113c3f5c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from itertools import chain\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pyjsparser import parse\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "URL = 'https://tracktranslegislation.com'\n",
    "page = requests.get(URL)\n",
    "soup = Soup(page.content, 'html.parser')\n",
    "script_tags = soup.find_all('script')\n",
    "sources = [source_tag.attrs['src'] for source_tag in soup.find_all('script') if source_tag.has_attr('src')]\n",
    "quarry = urljoin(URL, next(source for source in sources if 'chunks/70-' in source))\n",
    "script_contents = requests.get(quarry).text\n",
    "\n",
    "def find_in_graph(subgraph):\n",
    "    results = []\n",
    "    items = subgraph.values() if isinstance(subgraph, dict) else subgraph\n",
    "    local_results = (item for item in items if isinstance(item, str) and len(item) > 10000)\n",
    "    return (chain.from_iterable([local_results, *(find_in_graph(item) for item in items if isinstance(item, dict) or isinstance(item,list))]))\n",
    "\n",
    "parsed = parse(script_contents)\n",
    "candidates = find_graph(parsed)\n",
    "jsonstr = next(candidates)\n",
    "len(json.loads(jsonstr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d9735-dd29-4e0a-a9ec-8f18ee791954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = ('a', 3)\n",
    "t2 = (*t1, 'b')\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82488a43-ee59-4e96-83ae-5b677f49a35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain, islice\n",
    "import json\n",
    "\n",
    "mapper = {}\n",
    "with open('resolver_map.json', 'r') as f:\n",
    "    mapper = json.load(f)\n",
    "\n",
    "list(islice(chain.from_iterable(\n",
    "    ((state, k, v) for k, v in m['bills'].items())\n",
    "    for state, m \n",
    "    in mapper.items()\n",
    "), 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1601a-fe14-4330-aad1-9afcbcf53c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b3615-7776-475c-b672-21bbfa6ac2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mergedeep import merge\n",
    "\n",
    "a = {\n",
    "    'foo': {\n",
    "        'bar': 'baz',\n",
    "        'glarch': 'glarch',\n",
    "    },\n",
    "    'bar': 3,\n",
    "    'arr': [3]\n",
    "}\n",
    "\n",
    "b = {\n",
    "    'foo': {\n",
    "        'bar': 'bar',\n",
    "    },\n",
    "    'baz': 'baz',\n",
    "    'bar': {\n",
    "        'blah': 'blah'\n",
    "    },\n",
    "    'arr': [4]\n",
    "}\n",
    "\n",
    "c = {\n",
    "    'foo': { }\n",
    "}\n",
    "\n",
    "dummy = {}\n",
    "merge(dummy, a, b, c)\n",
    "pprint(dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da474ac1-ec67-4292-978c-b09e049c9c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from legiscan import legiscan_api\n",
    "import requests\n",
    "\n",
    "@legiscan_api\n",
    "def do_search(state: str, term: str, api_key: str):\n",
    "    short_url = 'https://api.legiscan.com/'\n",
    "    assemble_url = f'https://api.legiscan.com/?key={api_key}&op=getSearch&state={state}&query={term.replace(\" \", \"+\")}'\n",
    "    assemble_params = {\n",
    "        'key': api_key,\n",
    "        'op': 'getSearch',\n",
    "        'state': state,\n",
    "        'query': term,\n",
    "    }\n",
    "    return requests.get(short_url, params=assemble_params).text\n",
    "\n",
    "json.loads(do_search('ME', '577')) # needs to be exact or the search can't find it :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea651e3-960d-4083-a8a4-eb49e5012167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from itertools import islice\n",
    "from nltk import download as nltk_download, pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "nltk_download('punkt')\n",
    "nltk_download('averaged_perceptron_tagger')\n",
    "nltk_download('wordnet')\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def extract_html_file(file_path: str):\n",
    "    soup = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        soup = Soup(f, 'html.parser')\n",
    "\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    text = soup.get_text()\n",
    "    tokens = (token for token in word_tokenize(text) if token not in string.punctuation)\n",
    "    return list(tokens)\n",
    "\n",
    "[(orig, lem) for orig, lem in ((word, lem.lemmatize(word, get_wordnet_pos(pos))) for word, pos in pos_tag(extract_html_file('bills/FL_S0254.html'))) if orig != lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9694e04-952b-465b-81ab-2a9b8bf9f331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from itertools import islice\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def extract_html_file(file_path: str):\n",
    "    soup = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        soup = Soup(f, 'html.parser')\n",
    "\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    text = soup.get_text()\n",
    "    tokens = (token for token in word_tokenize(text) if token not in string.punctuation)\n",
    "    return tokens\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    #lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    #chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    #text = \"\\n\".join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    #return text\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "pprint([(word, ps.stem(word)) for word in islice(extract_html_file('bills/FL_S0254.html'), 100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c39bdf-ec41-4f0e-b4c2-dd8633109951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import base64\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "filename = 'tmp/bill_meta_TX_HB1532.json'\n",
    "\n",
    "result = None\n",
    "with open(filename, 'r') as f:\n",
    "    result = json.load(f)['bill']\n",
    "\n",
    "extra = result['texts'][0].copy()\n",
    "extra['date'] = '2023-01-01'\n",
    "result['texts'].append(extra)\n",
    "# pprint(result['texts'])\n",
    "s = sorted(result['texts'], key=lambda x: x['date'], reverse=True)\n",
    "doc_id = result['texts'][0]['doc_id']\n",
    "doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77109c-79ed-4fd1-a625-bb7b138dfcde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from legiscan import legiscan_api\n",
    "import pandas as pd\n",
    "\n",
    "raw = pd.read_json('tracktranslegislation.json')\n",
    "sample = raw.sample(n=3, random_state=1234)\n",
    "\n",
    "@legiscan_api\n",
    "def get_bill_meta(legiscan_bill_id: str, api_key: str):\n",
    "    assembled_url = f'https://api.legiscan.com/?key={api_key}&op=getBill&id={legiscan_bill_id}'\n",
    "    resp = requests.get(assembled_url)\n",
    "\n",
    "    if resp.ok:\n",
    "        parsed = json.loads(resp.text)\n",
    "        if parsed['status'] == 'ERROR':\n",
    "            print(parsed['alert']['message'])\n",
    "            return None\n",
    "        return resp\n",
    "    else:\n",
    "        print(resp.status_code)\n",
    "        return None\n",
    "    \n",
    "for idx, row in sample.iterrows():\n",
    "    local_filename = os.path.join(\n",
    "        'tmp',\n",
    "        '_'.join([\n",
    "            'bill',\n",
    "            'meta',\n",
    "            row[\"state\"],\n",
    "            *row[\"billId\"].split(' '),\n",
    "        ])\n",
    "    ) + '.json'\n",
    "    \n",
    "    if os.path.exists(local_filename):\n",
    "        print(f'skipping {local_filename}')\n",
    "        continue\n",
    "    \n",
    "    resp = None\n",
    "    resp = get_bill_meta(row['legiscanId'])\n",
    "    if not resp:\n",
    "        print(f'Could not download {local_filename}')\n",
    "        continue\n",
    "    \n",
    "    print(f'got {local_filename}')\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        f.write(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180872e-a65b-4289-bb41-8cff73e0dc0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw = pd.read_json('tracktranslegislation.json')\n",
    "sample = raw.sample(n=3, random_state=1234)\n",
    "sample\n",
    "\n",
    "# 7 has wrong legiscanId, should be 2721785 i think\n",
    "# 44 also has wrong legiscanId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1612b-bdd2-4c19-9419-6280b93d021f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "filename = 'bills/TX_HB1532'\n",
    "\n",
    "result = None\n",
    "with open(filename, 'r') as f:\n",
    "    result = json.load(f)['text']\n",
    "\n",
    "doc = result['doc']\n",
    "extension = result['mime'].split('/')[-1]\n",
    "new_filename = '.'.join([filename, extension])\n",
    "\n",
    "with open(new_filename, 'wb') as f:\n",
    "    f.write(base64.b64decode(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fff686-1f74-4e15-a9a1-19d27706da9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from legiscan import legiscan_api\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "@legiscan_api\n",
    "def get_bill_text(legiscan_bill_id: str, api_key: str):\n",
    "    # https://api.legiscan.com/?key=5f61f50916512f9f21500f38877c22f7&op=getBillText&id=2736883\n",
    "    assembled_url = f'https://api.legiscan.com/?key={api_key}&op=getBillText&id={legiscan_bill_id}'\n",
    "    resp = requests.get(assembled_url)\n",
    "\n",
    "    if resp.ok:\n",
    "        parsed = json.loads(resp.text)\n",
    "        if parsed['status'] == 'ERROR':\n",
    "            print(parsed['alert']['message'])\n",
    "            return None\n",
    "        return resp\n",
    "    else:\n",
    "        print(resp.status_code)\n",
    "        return None\n",
    "\n",
    "raw = pd.read_json('tracktranslegislation.json')\n",
    "sample = raw.sample(n=3, random_state=1234)\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    local_filename = os.path.join(\n",
    "        'bills',\n",
    "        '_'.join([\n",
    "            row[\"state\"],\n",
    "            *row[\"billId\"].split(' '),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    if os.path.exists(local_filename):\n",
    "        print(f'skipping {local_filename}')\n",
    "    \n",
    "    resp = get_bill_text(row['legiscanId'])\n",
    "    if not resp:\n",
    "        print(f'Could not download {local_filename}')\n",
    "        continue\n",
    "    \n",
    "    with open(local_filename, 'wb') as f:\n",
    "        f.write(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77354a2b-b3bc-4f96-bf92-2dbc7a80279e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from legiscan import legiscan_api\n",
    "import os\n",
    "\n",
    "@legiscan_api\n",
    "def sample_api_action(api_key: str):\n",
    "    print(f'api key is {api_key}')\n",
    "\n",
    "@legiscan_api\n",
    "def get_bill_text(legiscan_bill_id: str, api_key: str):\n",
    "    # https://api.legiscan.com/?key=5f61f50916512f9f21500f38877c22f7&op=getBillText&id=2736883\n",
    "    assembled_url = f'https://api.legiscan.com/?key={api_key}&op=getBillText&id={legiscan_bill_id}'\n",
    "    print(assembled_url)\n",
    "    \n",
    "sample_api_action(api_key='foo')\n",
    "\n",
    "\n",
    "get_bill_text('12345')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec9fc5-e6b0-4f04-a00e-097f4097fbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "host = 'https://www.house.mo.gov'\n",
    "# url = f'{host}/BillContent.aspx?bill=HB1258&year=2023&code=R'\n",
    "url = urllib.parse.urljoin(host, 'BillContent.aspx?bill=HB1258&year=2023&code=R')\n",
    "page = requests.get(url)\n",
    "\n",
    "# print(page.text)\n",
    "soup = BeautifulSoup(page.content)\n",
    "urllib.parse.urljoin(\n",
    "    host, \n",
    "    soup.find_all(class_='textType')[0].find('a')['href'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0164bd3-1c4f-4ebc-b7a4-5eaa81af950b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Optional\n",
    "import urllib.parse\n",
    "\n",
    "def test_url(url: str, parent_id: str, anchor_index: int):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    container = soup.find(id=parent_id)\n",
    "    if not container:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return container.find_all('a')[anchor_index]['href']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def prepare_url(relative_url: Optional[str]):\n",
    "    if not relative_url:\n",
    "        return 'NO RESULT'\n",
    "    \n",
    "    return urllib.parse.urljoin('https://legiscan.com/', relative_url)\n",
    "\n",
    "def process_as_bill(frame) -> Optional[str]:\n",
    "    return process_bill_link(frame['billLink'])\n",
    "\n",
    "def process_bill_link(url: str) -> Optional[str]:\n",
    "    print(url)\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        container = soup.find(id='bill-last-action')\n",
    "        anchors = container.find_all('a')\n",
    "        href = anchors[-1]['href']\n",
    "        return urllib.parse.urljoin('https://legiscan.com', href)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_as_text(frame) -> Optional[str]:\n",
    "    state = frame['state']\n",
    "    bill_id = frame['billId'].replace(' ', '')\n",
    "    year = row['billLink'].split('/')[-1] # don't trust statusDate\n",
    "    text_link = f'https://legiscan.com/{state}/text/{bill_id}/{year}'\n",
    "    return process_text_link(text_link)\n",
    "\n",
    "def process_text_link(url: str) -> Optional[str]:\n",
    "    print(url)\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        container = soup.find(id='gaits-wrapper')\n",
    "        href = container.find_all('a')[-1]['href']\n",
    "        return urllib.parse.urljoin('https://legiscan.com', href)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_as_draft(frame) -> Optional[str]:\n",
    "    state = frame['state']\n",
    "    bill_id = frame['billId'].replace(' ', '')\n",
    "    year = row['billLink'].split('/')[-1] # don't trust statusDate\n",
    "    text_link = f'https://legiscan.com/{state}/drafts/{bill_id}/{year}'\n",
    "    return process_text_link(text_link)\n",
    "    \n",
    "def process_draft_link(url: str) -> Optional[str]:\n",
    "    print(url)\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        container = soup.find(id='gaits-wrapper')\n",
    "        href = container.find_all('a')[-1]['href']\n",
    "        return urllib.parse.urljoin('https://legiscan.com', href)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "raw = pd.read_json('tracktranslegislation.json')\n",
    "sample = raw.copy()\n",
    "# sample = raw.sample(n=20, random_state=2)\n",
    "# sample = raw.loc[raw.state == 'AR']\n",
    "# sample = raw.loc[0:20]\n",
    "# print(sample)\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    continue\n",
    "    bill_id = ' '.join([row['state'], row['billId']])\n",
    "#    year = row['billLink'].split('/')[-1]\n",
    "    \n",
    "#    bill_link = row['billLink']\n",
    "#    draft_link = f'https://legiscan.com/{row[\"state\"]}/drafts/{row[\"billId\"].replace(\" \", \"\")}/{year}' # https://legiscan.com/AZ/drafts/HB2517/2023\n",
    "#    text_link = f'https://legiscan.com/{row[\"state\"]}/text/{row[\"billId\"].replace(\" \", \"\")}/{year}'\n",
    "#    comments_link = f'https://legiscan.com/{row[\"state\"]}/comments/{row[\"billId\"].replace(\" \", \"\")}/{year}'\n",
    "    \n",
    "    searches = [\n",
    "        process_as_bill,\n",
    "        process_as_text,\n",
    "    ]\n",
    "\n",
    "    print(f'{bill_id}')\n",
    "    for search in searches:\n",
    "        print(search(row))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44ff23-fac6-4b06-acae-4fcaa34bdd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://legiscan.com/TX/comments/HB1029/2023\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from typing import Optional\n",
    "import urllib.parse\n",
    "\n",
    "def process_as_bill(frame) -> Optional[str]:\n",
    "    return process_bill_link(frame['billLink'])\n",
    "\n",
    "def process_bill_link(url: str) -> Optional[str]:\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        container = soup.find(id='bill-last-action')\n",
    "        anchors = container.find_all('a')\n",
    "        href = anchors[-1]['href']\n",
    "        return urllib.parse.urljoin('https://legiscan.com', href)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_as_text(frame) -> Optional[str]:\n",
    "    state = frame['state']\n",
    "    bill_id = frame['billId'].replace(' ', '')\n",
    "    year = row['billLink'].split('/')[-1] # don't trust statusDate\n",
    "    text_link = f'https://legiscan.com/{state}/text/{bill_id}/{year}'\n",
    "    return process_text_link(text_link)\n",
    "\n",
    "def process_text_link(url: str) -> Optional[str]:\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        container = soup.find(id='gaits-wrapper')\n",
    "        href = container.find_all('a')[-1]['href']\n",
    "        return urllib.parse.urljoin('https://legiscan.com', href)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_as_draft(frame) -> Optional[str]:\n",
    "    state = frame['state']\n",
    "    bill_id = frame['billId'].replace(' ', '')\n",
    "    year = row['billLink'].split('/')[-1] # don't trust statusDate\n",
    "    text_link = f'https://legiscan.com/{state}/drafts/{bill_id}/{year}'\n",
    "    return process_text_link(text_link)\n",
    "    \n",
    "def process_draft_link(url: str) -> Optional[str]:\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content)\n",
    "        container = soup.find(id='gaits-wrapper')\n",
    "        href = container.find_all('a')[-1]['href']\n",
    "        return urllib.parse.urljoin('https://legiscan.com', href)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "#text_link = 'https://legiscan.com/TX/text/HB3147/2023'\n",
    "#print(text_link)\n",
    "#print(process_text_link(text_link))\n",
    "\n",
    "#bill_link = 'https://legiscan.com/TX/bill/HB976/2023'\n",
    "#print(bill_link)\n",
    "#print(process_bill_link(bill_link))\n",
    "#print(process_as_bill(sample.loc[303]))\n",
    "\n",
    "print(process_as_bill(raw.loc[251]))\n",
    "print(process_as_text(raw.loc[251]))\n",
    "print(process_as_draft(raw.loc[251]))\n",
    "\n",
    "#host = 'https://www.house.mo.gov'\n",
    "# url = f'{host}/BillContent.aspx?bill=HB1258&year=2023&code=R'\n",
    "#url = 'https://legiscan.com/SD/text/HB1080/2023' #urllib.parse.urljoin(host, 'BillContent.aspx?bill=HB1258&year=2023&code=R')\n",
    "#page = requests.get(url)\n",
    "\n",
    "# print(page.text)\n",
    "#soup = BeautifulSoup(page.content)\n",
    "#container = soup.find(id='gaits-wrapper')\n",
    "#pprint(container.find_all('a'))\n",
    "#urllib.parse.urljoin(\n",
    "#    host, \n",
    "#    soup.find_all(class_='textType')[0].find('a')['href'],\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
